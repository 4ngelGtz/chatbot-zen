# ğŸ“š Chatbot-Zen

Chatbot-Zen is an **end-to-end Retrieval-Augmented Generation (RAG) demo** that ingests YouTube transcripts, indexes them with FAISS, and serves answers through a FastAPI backend. The project is modular, reproducible, and CI-ready.

---

## ğŸš€ Current Progress

### âœ… Phase 1 â€“ Repo & Setup
- Project repository created.
- Professional README in English with roadmap and badges.
- Virtual environment managed with `make install` / `make reinstall`.

### âœ… Phase 2 â€“ Data Ingestion
- **Video IDs** fetched via `src/fetch_video_ids.py`.
- **Transcripts** downloaded via `src/fetch_transcripts.py`:
  - Uses `yt-dlp` (with cookies and impersonation).
  - Prefers `en-US`, falls back to `en`, then translates to `en` with `youtube-transcript-api`.
  - Stores subtitles in `data/raw/subs/<VIDEO_ID>/Transcript [VIDEO_ID].<lang>.srt`.
  - Generates an index in `data/chunks/transcripts_index.jsonl`.
- **Problems fixed**:
  - 429 errors handled with retries, backoff, impersonation, and cookies.
  - Deterministic file names for stable indexing.

### âœ… Phase 3 â€“ Backend RAG
- Core pipeline ready:
  - `src/utils.py`
  - `src/rag_pipeline.py` â†’ FAISS + OpenAI/HF embeddings
  - `src/main.py` â†’ FastAPI server
- Endpoints:
  - `GET /health` â†’ `{status:"ok"}`
  - `POST /ask` â†’ `{answer, sources, provider, top_k}`
- `.env` config for choosing embeddings/LLM providers.

### âœ… Testing & CI
- Unit test: `tests/test_transcripts_enus.py` ensures transcripts can be downloaded for a reference video.
- `make test` runs all tests.
- `make test-enus` runs the smoke test on a single video.
- GitHub Actions workflow (`.github/workflows/test.yml`) runs tests on every push/PR.

---

## ğŸ“‚ Project Structure

```bash
chatbot-zen/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ test.yml              # CI workflow (pytest + yt-dlp master)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ subs/                 # transcripts per video (srt files)
â”‚   â”œâ”€â”€ chunks/
â”‚   â”‚   â””â”€â”€ transcripts_index.jsonl
â”‚   â””â”€â”€ video_ids.jsonl           # list of video IDs (fetched)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ fetch_video_ids.py        # fetch list of videos
â”‚   â”œâ”€â”€ fetch_transcripts.py      # download transcripts for all videos
â”‚   â”œâ”€â”€ make_langchain_docs.py    # (next) convert transcripts to LC docs
â”‚   â”œâ”€â”€ rag_pipeline.py           # FAISS + embeddings pipeline
â”‚   â”œâ”€â”€ utils.py                  # helper functions
â”‚   â””â”€â”€ main.py                   # FastAPI app
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_transcripts_enus.py  # transcript smoke test
â”œâ”€â”€ .env                          # environment variables (providers, cookies, channel URL)
â”œâ”€â”€ requirements.txt              # dependencies (yt-dlp, transcript API, dotenv, tqdm, pytest)
â”œâ”€â”€ Makefile                      # build/test/clean tasks
â””â”€â”€ README.md                     # this file


## ğŸ› ï¸ Makefile Targets
make install          # create venv + install dependencies
make reinstall        # recreate venv from scratch
make ids              # fetch video IDs
make transcripts      # fetch transcripts for all videos
make retranscripts    # clean + re-fetch all transcripts
make clean            # clean data directory
make test             # run all tests
make test-enus        # run smoke test on reference video
make use-ytdlp-master # upgrade yt-dlp to latest master
make ytdlp-version    # check yt-dlp version and impersonation support



## ğŸ› ï¸ Roadmap
- [x] Phase 1 â€“ Repo setup & design  
- [x] Phase 2 â€“ YouTube transcript ingestion  
- [ ] Phase 3 â€“ RAG pipeline + FastAPI backend  
- [ ] Phase 4 â€“ Dockerization & CI/CD  
- [ ] Phase 5 â€“ Cloud deployment (GCP/Azure/AWS)  
- [ ] Phase 6 â€“ Frontend integration (Streamlit/React)  
- [ ] Phase 7 â€“ Monitoring & optimization  

---

## ğŸ“œ License
MIT License â€“ free to use and adapt.
